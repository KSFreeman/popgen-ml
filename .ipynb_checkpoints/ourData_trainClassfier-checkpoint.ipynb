{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/anaconda3/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/home/kevin/anaconda3/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/kevin/anaconda3/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import cross_validation\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn import svm\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainingSetDir, classifierPickleFileName = sys.argv[1:3]\n",
    "# statsToUse = sys.argv[3:]\n",
    "trainingSetDir           = \"/media/kevin/TOSHIBA_EXT/classify_chroms/feature_vecs/15win\"         #sys.argv[1]\n",
    "classifierPickleFileName = \"/media/kevin/TOSHIBA_EXT/classify_chroms/classifiers/15winClf.p\" #sys.argv[2]\n",
    "statsToUse               = \"all\"         #sys.argv[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classList = []\n",
    "trainingData = []\n",
    "labelToClassName = {}\n",
    "headerH = {}\n",
    "for trainingSetFileName in os.listdir(trainingSetDir):\n",
    "    classList.append(trainingSetFileName.split(\".fvec\")[0])\n",
    "    trainingSetFile = open(trainingSetDir + \"/\" + trainingSetFileName)\n",
    "    currTrainingData = trainingSetFile.readlines()\n",
    "    trainingSetFile.close()\n",
    "\n",
    "    trainingData += currTrainingData[1:]#append all training data from the current set (minus the header)\n",
    "\n",
    "    currLabelH = {}\n",
    "    for example in currTrainingData[1:]:\n",
    "        currLabelH[example.split(\"\\t\")[0]] = 1\n",
    "    assert len(currLabelH) == 1\n",
    "    labelToClassName[currLabelH.keys()[0]] = trainingSetFileName.split(\".fvec\")[0]\n",
    "\n",
    "    header = currTrainingData[0].strip().split(\"\\t\")\n",
    "    headerH[currTrainingData[0].strip()] = 1\n",
    "    assert header[0] == \"classLabel\"\n",
    "    statIndices = []\n",
    "    if \"all\" in statsToUse:\n",
    "        statIndices = range(1, len(header))\n",
    "    else:\n",
    "        for i in range(1, len(header)):\n",
    "            if header[i] in statsToUse or header[i].split(\"_win\")[0] in statsToUse:\n",
    "                statIndices.append(i)\n",
    "assert len(headerH) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1418 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using these features: all (indices: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n",
      "training set size after balancing: 420\n"
     ]
    }
   ],
   "source": [
    "sys.stderr.write(\"using these features: %s (indices: %s)\\n\" %(str(statsToUse), str(statIndices)))\n",
    "XH = {}\n",
    "for i in range(len(trainingData)):\n",
    "    trainingData[i] = trainingData[i].strip().split(\"\\t\")\n",
    "    currVector = []\n",
    "    if not \"nan\" in trainingData[i]:\n",
    "        for j in statIndices:\n",
    "            currVector.append(float(trainingData[i][j]))\n",
    "        assert len(currVector) == len(statIndices)\n",
    "        if not XH.has_key(trainingData[i][0]):\n",
    "            XH[trainingData[i][0]] = []\n",
    "        XH[trainingData[i][0]].append(currVector)\n",
    "\n",
    "#balance the training set\n",
    "minClassSize = min([len(XH[classLabel]) for classLabel in  XH.keys()])\n",
    "X = []\n",
    "y = []\n",
    "for classLabel in sorted(XH.keys()):\n",
    "    random.shuffle(XH[classLabel])\n",
    "    for i in range(minClassSize):\n",
    "        currVector = XH[classLabel][i]\n",
    "        X.append(currVector)\n",
    "        y.append(classLabel)\n",
    "sys.stderr.write(\"training set size after balancing: %s\\n\" %(len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking accuracy when distinguishing among all 7 classes\n"
     ]
    }
   ],
   "source": [
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "sys.stderr.write(\"Checking accuracy when distinguishing among all %s classes\\n\" %(len(XH.keys())))\n",
    "\n",
    "maxMaxFeatures = len(X[0])\n",
    "param_grid_forest = {\"max_depth\": [3, 10, None],\n",
    "              \"max_features\": [1, 3, int(maxMaxFeatures**0.5), maxMaxFeatures],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training extraTreesClassifier\n"
     ]
    }
   ],
   "source": [
    "clf, mlType, paramGrid = ExtraTreesClassifier(n_estimators=100), \"extraTreesClassifier\", param_grid_forest\n",
    "\n",
    "heatmap = []\n",
    "sys.stderr.write(\"Training %s\\n\" %(mlType))\n",
    "grid_search = GridSearchCV(clf,param_grid=param_grid_forest,cv=10,n_jobs=1)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "sys.stderr.write(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\\n\"\n",
    "      % (time() - start, len(grid_search.grid_scores_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for extraTreesClassifier\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.743 (std: 0.047)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'min_samples_split': 2, 'criterion': 'gini', 'max_features': 99, 'max_depth': 10}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.717 (std: 0.077)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'min_samples_split': 10, 'criterion': 'gini', 'max_features': 99, 'max_depth': 10}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.712 (std: 0.056)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'min_samples_split': 3, 'criterion': 'gini', 'max_features': 9, 'max_depth': None}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/media/kevin/TOSHIBA_EXT/classify_chroms/classifiers/origClf.p']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Results for %s\" %(mlType)\n",
    "report(grid_search.grid_scores_)\n",
    "joblib.dump((labelToClassName, classList, statsToUse, header, grid_search), classifierPickleFileName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
